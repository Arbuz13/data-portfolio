# Predictive Maintenance â€” Multiclass Failure Classification (XGBoost Â· Random Forest Â· SHAP Â· Streamlit)

This project builds a **Predictive Maintenance (PdM)** pipeline that classifies **machine failure types** from sensor data.
It covers **EDA â†’ preprocessing â†’ feature engineering â†’ modeling (LR / RF / XGB) â†’ class imbalance (SMOTE) â†’ tuning â†’ SHAP explainability â†’ deployment (Streamlit)**.

> **Goal:** reduce unplanned downtime by accurately predicting failure types and providing interpretable insights for maintenance teams.

---

## ğŸ“¦ Project Highlights

- **Two tasks:** Binary (Failure vs No Failure) and **Multiclass** (6 classes: No Failure, Heat Dissipation, Power, Overstrain, Tool Wear, Random).
- **Models:** Logistic Regression (baseline), **Random Forest**, **XGBoost** (best overall).
- **Preprocessing:** outlier capping (Winsorization), **feature scaling**, and **feature engineering** (`temperature_diff = process - air`).
- **Imbalance handling:** **SMOTE** for minority classes.
- **Explainability:** **SHAP** global & local explanations (summary, violin, force/waterfall).
- **Deployment:** **Streamlit app** with on-demand prediction + SHAP waterfall for the predicted class.

---

## ğŸ”„ Methodology

**Ingest â†’ EDA â†’ Clean â†’ Engineer â†’ Scale/Split â†’ Train â†’ Tune â†’ Explain â†’ Deploy**

![Methodology](./docs/methodology.png)
*Figure 1: Methodology flow (replace with your diagram).*

**Key steps**
1. Load dataset; audit types, nulls, duplicates.
2. Remove non-predictive identifiers; encode categorical `Type (L/M/H) â†’ Type_encoded`.
3. **Winsorize** outliers for `Rotational speed [rpm]` and `Torque [Nm]`.
4. Engineer **`temperature_diff = Process temperature - Air temperature`**.
5. **Scale** numeric features; **train/test split** (80/20).
6. Train LR/RF/XGB for **binary** & **multiclass**; handle class imbalance with **SMOTE**.
7. **Tune** (RandomizedSearchCV) RF/XGB; evaluate with accuracy, precision, recall, F1, ROC.
8. **Explain** predictions using **SHAP**.
9. Package best **XGBoost** as a **joblib pipeline** and deploy with **Streamlit** (`app/app.py`).

---

## ğŸ— Data & Features

- **Inputs:** `Air temperature [K]`, `Process temperature [K]`, `Rotational speed [rpm]`, `Torque [Nm]`, `Tool wear [min]`, `Type_encoded`
- **Engineered:** `temperature_diff = Process temperature - Air temperature`
- **Targets:**
  - **Binary:** `Target` (0 = No Failure, 1 = Failure)
  - **Multiclass:** `Failure type` â†’ `{0..5}` (No Failure, Heat Dissipation, Power, Overstrain, Tool Wear, Random)

---

## ğŸ“ˆ Results (from dissertation/experiments)

- **Binary classification**
  - **Random Forest / XGBoost:** **Accuracy â‰ˆ 0.9845**, strong recall for failure class
  - Logistic Regression: high accuracy but low failure recall
- **Multiclass classification (after tuning)**
  - **XGBoost:** **Accuracy â‰ˆ 0.97**
  - **Random Forest:** Accuracy â‰ˆ 0.94
- **Top features (SHAP):** **Torque [Nm]**, **Tool wear [min]**, **Rotational speed [rpm]**, **temperature_diff**

> *Exact confusion matrices, ROC, and SHAP plots should be exported to `docs/` and referenced below.*

---

## ğŸ–¼ Figures & Diagrams

- **Methodology:** `./docs/methodology.png`
- **EDA:** distributions, outliers before/after winsorization (`./docs/eda_*.png`)
- **Confusion matrices & ROC:** `./docs/confusion_*.png`, `./docs/roc_*.png`
- **SHAP:** `./docs/shap_summary.png`, `./docs/shap_violin.png`, `./docs/shap_force.png` (or waterfall)
- **Streamlit:** app screenshots `./docs/streamlit_ui.png`, `./docs/streamlit_pred.png`

> Place your images in the `docs/` folder using the filenames above so they render here.

---

## ğŸ—‚ Repository Structure

```
predictive-maintenance/
â”œâ”€ README.md
â”œâ”€ app/
â”‚  â””â”€ app.py                           # Streamlit app (uses models/xgb_multiclass_pipeline.pkl)
â”œâ”€ models/
â”‚  â””â”€ .gitkeep                         # place trained pipeline here (joblib)
â”œâ”€ notebooks/
â”‚  â””â”€ my_project_pdm.ipynb            # full notebook (outputs cleared if large)
â”œâ”€ scripts/
â”‚  â”œâ”€ train_xgb.py                     # training scaffold to reproduce the pipeline
â”‚  â””â”€ requirements.txt                 # minimal environment
â”œâ”€ docs/
â”‚  â”œâ”€ methodology.png                  # Fig 1
â”‚  â”œâ”€ eda_distributions.png
â”‚  â”œâ”€ eda_outliers_before.png
â”‚  â”œâ”€ eda_outliers_after.png
â”‚  â”œâ”€ confusion_binary.png
â”‚  â”œâ”€ roc_binary.png
â”‚  â”œâ”€ confusion_multiclass.png
â”‚  â”œâ”€ roc_multiclass.png
â”‚  â”œâ”€ shap_summary.png
â”‚  â”œâ”€ shap_violin.png
â”‚  â”œâ”€ shap_force.png
â”‚  â”œâ”€ streamlit_ui.png
â”‚  â””â”€ streamlit_pred.png
â””â”€ data/
   â””â”€ README.md                       # put sample or synthetic data here if needed
```

---

## ğŸ§ª Reproduce: Train the XGBoost Multiclass Model

```bash
# 1) Create & activate a venv (recommended)
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 2) Install deps
pip install -r scripts/requirements.txt

# 3) Train & export joblib pipeline
python scripts/train_xgb.py --data ./data/predictive_maintenance.csv --out ./models/xgb_multiclass_pipeline.pkl
```

**Notes**
- Ensure your CSV columns match the feature names in this repo.
- The training script performs: load â†’ clean â†’ winsorize â†’ encode â†’ scale â†’ SMOTE â†’ train â†’ save pipeline.

---

## â–¶ï¸ Run the Streamlit App

```bash
# In the project root
streamlit run app/app.py
```
The app expects `./models/xgb_multiclass_pipeline.pkl`. After training, ensure the file exists at that path.

---

## ğŸ” Model Explainability (SHAP)

The app computes a **SHAP waterfall** for the predicted class on the provided input.  
Export your **global SHAP** plots (summary/violin) from the notebook into `./docs/` and reference them here.

---

## ğŸ“„ Data & Ethics

- Use only **public/synthetic** data in this repo.
- Never commit secrets or sensitive data.
- Document assumptions and preprocessing choices for reproducibility.

---

## âœ… Credits

- Dissertation/Report: *Predictive Maintenance in Manufacturing Industries: Leveraging Machine Learning for Operational Efficiency* (2025)
- Author: **Williams Providence Ogheneruese**

_Last updated: 07 Sep 2025_
